{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./images/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternating Least Squares with Real Data\n",
    "\n",
    "Since we already have practice working with sparse data, we can apply the same techniques to our Amazon dataset. The trick is that because the data is so large, it will be trickier to use our debugging techniques from the <a href=\"1-05_als.ipynb\"></a>previous notebook.\n",
    "\n",
    "## Objectives\n",
    "This notebook demonstrates:\n",
    "* How to build a collaborative filter for large datasets\n",
    "    * [1. Splitting into Train and Test](#1.-Splitting-into-Train-and-Test)\n",
    "    * [2. Alternating Least Squares](#2.-Alternating-Least-Squares)\n",
    "    * [3. Wrap Up](#3.-Wrap-Up)\n",
    "\n",
    "## 1. Splitting into Train and Test    \n",
    "We already did data cleansing in an earlier notebook, so let's load it up along with our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2WUGQKN584PCJ</td>\n",
       "      <td>B00001P4ZH</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1BEMKOWN1TJK4</td>\n",
       "      <td>B00001P4ZH</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2IEKS5Z84WVSR</td>\n",
       "      <td>B00001P4ZH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2X6CTUCGR89UU</td>\n",
       "      <td>B00001P4ZH</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1IDFR483R23N</td>\n",
       "      <td>B00001P4ZH</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall  valid\n",
       "0  A2WUGQKN584PCJ  B00001P4ZH      5.0  False\n",
       "1  A1BEMKOWN1TJK4  B00001P4ZH      4.0  False\n",
       "2  A2IEKS5Z84WVSR  B00001P4ZH      1.0  False\n",
       "3  A2X6CTUCGR89UU  B00001P4ZH      5.0  False\n",
       "4   A1IDFR483R23N  B00001P4ZH      4.0  False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import cupyx.scipy.sparse\n",
    "import cudf\n",
    "import numpy as np\n",
    "\n",
    "ratings = cudf.read_csv(\"data/ratings.csv\")\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we split into train and test, let's [factorize](https://docs.rapids.ai/api/cudf/stable/api_docs/api/cudf.Series.factorize.html) our users and items. Our test set won't be effective if it doesn't use the same indexes as the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indexes, user_mapping = ratings[\"reviewerID\"].factorize()\n",
    "item_indexes, item_mapping = ratings[\"asin\"].factorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're building our sparse matrix, let's get a sense of our data along the way. First, let's see how many users there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192403"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_count = user_mapping.count()\n",
    "\n",
    "user_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost two hundred thousand! Okay, how about the items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_count = item_mapping.count()\n",
    "\n",
    "item_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much bigger than our toy problem with four users and five items. Using [todense](https://docs-cupy.chainer.org/en/stable/reference/generated/cupyx.scipy.sparse.coo_matrix.html#cupyx.scipy.sparse.coo_matrix.todense) will result in memory limitations, which in the real world is both frustrating and expensive.\n",
    "\n",
    "Instead, we'll need to rely on good testing. Let's break our ratings data down into train and test. Please fill in the `FIXME`s below, using the <a href=\"1-05_als_intro.ipynb\">previous lab</a> as a hint if needed. There is one `FIXME` for each input of the `get_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5a4d5edb6748>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m train_row, train_column, train_data, train_sparse, train_mask = get_dataset(\n\u001b[0;32m---> 19\u001b[0;31m     train_indexes, user_indexes, item_indexes, overall)\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m valid_row, valid_column, valid_data, valid_sparse, valid_mask = get_dataset(\n",
      "\u001b[0;32m<ipython-input-8-5a4d5edb6748>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(data_selector, user_indexes, item_indexes, overall)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# FIXME: build sparse matrix with correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msparse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupyx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msparse_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupyx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/cupyx/scipy/sparse/coo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'row index exceeds matrix dimensions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.ndarray.__nonzero__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "train_indexes = ~ratings[\"valid\"]\n",
    "valid_indexes = ratings[\"valid\"]\n",
    "overall = ratings[\"overall\"]\n",
    "\n",
    "def get_dataset(data_selector, user_indexes, item_indexes, overall):\n",
    "    # FIXME: find als rows and columns based on indexes\n",
    "    row = cp.asarray(user_indexes[data_selector])\n",
    "    column = cp.asarray(item_indexes[data_selector])\n",
    "    data = cp.asarray(overall[data_selector])\n",
    "    \n",
    "    # FIXME: build sparse matrix with correct shape\n",
    "    sparse_data = cupyx.scipy.sparse.coo_matrix((data, (row, column)), shape=(row, column))\n",
    "    mask = cp.asarray([1 for _ in range(len(data))], dtype=np.float32)\n",
    "    sparse_mask = cupyx.scipy.sparse.coo_matrix((mask, (row, column)), shape=(row, column))\n",
    "    return row, column, data, sparse_data, sparse_mask\n",
    "\n",
    "train_row, train_column, train_data, train_sparse, train_mask = get_dataset(\n",
    "    train_indexes, user_indexes, item_indexes, overall)\n",
    "\n",
    "valid_row, valid_column, valid_data, valid_sparse, valid_mask = get_dataset(\n",
    "    valid_indexes, user_indexes, item_indexes, overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Alternating Least Squares\n",
    "\n",
    "It's time to initialize our embeddings. Let's copy over our functions from the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = 2\n",
    "shape = (user_count, item_count)\n",
    "\n",
    "def initalize_features(length, embeddings):\n",
    "    return cp.random.rand(embeddings, length) * 2 - 1\n",
    "\n",
    "user_features = initalize_features(shape[0], embeddings)\n",
    "item_features = initalize_features(shape[1], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(user_features, item_features, data, row, column):\n",
    "    predictions = (user_features[:, row] * item_features[:, column]).sum(axis=0)\n",
    "    mean_squared_error = ((predictions - data) ** 2).mean() ** 0.5\n",
    "    \n",
    "    return predictions, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ALS function below is incomplete. Please fill in the `FIXME`s to get it up and running. There is one `FIXME` for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als(values, mask, features, scale=0.01):\n",
    "    # FIXME: add inputs to als algorithm\n",
    "    numerator = FIXME_A.dot(FIXME_B.T)\n",
    "    squared_features = (FIXME_B ** 2).sum(axis=0)\n",
    "    denominator = FIXME_C + FIXME_D.dot(squared_features)\n",
    "\n",
    "    return (numerator / denominator[:, None]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How low can you go? Run the below cell multiple times to watch the RMSE fall. If the above `FIXME`s are correctly implemented, the RMSE can reach below `1.25`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    user_features = als(train_sparse, train_mask, item_features)\n",
    "    item_features = als(train_sparse.T, train_mask.T, user_features)\n",
    "    predictions, error = rmse(\n",
    "        user_features, item_features, valid_data, valid_row, valid_column)\n",
    "\n",
    "    print (\"RMSE:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a human eyeball test to verify our prediction error. Do the predictions follow the same ranking order as the true ratings do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wrap Up\n",
    "\n",
    "To save our work, let's group the notebooks in a zip file by running the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r lab1_work.zip . -i '1*.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the zip file, right click on it from the file menu to the left and select \"Download\". The file menu may need to be refreshed by clicking the arrow circle above the list.\n",
    "\n",
    "Congratulations on finishing this set of notebooks! We learned how to do two major types of Recommender Systems: Content-based and Collaborative. While we were able to achieve a lower RMSE with Collaborative Filtering, there are some situations were Content-based wins out. Have an idea what they are? Head back over to the notebook launcher to take a quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./images/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
