{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"../images/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the Solution File for this notebook. Please try the [Challenge Version](../1-06_als_real_data.ipynb) first.**\n",
    "\n",
    "# Alternating Least Squares with Real Data\n",
    "\n",
    "Since we already have practice working with sparse data, we can apply the same techniques to our Amazon dataset. The trick is that because the data is so large, it will be trickier to use our debugging techniques from the <a href=\"1-05_als.ipynb\"></a>previous notebook.\n",
    "\n",
    "## Objectives\n",
    "This notebook demonstrates:\n",
    "* How to build a collaborative filter for large datasets\n",
    "    * [1. Splitting into Train and Test](#1.-Splitting-into-Train-and-Test)\n",
    "    * [2. Alternating Least Squares](#2.-Alternating-Least-Squares)\n",
    "    * [3. Wrap Up](#3.-Wrap-Up)\n",
    "\n",
    "## 1. Splitting into Train and Test    \n",
    "We already did data cleansing in an earlier notebook, so let's load it up along with our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AEVNTIQFU2TQ6</td>\n",
       "      <td>B00004ZCB3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1XI6NT41B6E6X</td>\n",
       "      <td>B00004ZCB3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2PMIMM3U3THM7</td>\n",
       "      <td>B00004ZCB3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1E9NFMLMVY61H</td>\n",
       "      <td>B00004ZCB4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AXABTEYS7A4A8</td>\n",
       "      <td>B00004ZCB4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall  valid\n",
       "0   AEVNTIQFU2TQ6  B00004ZCB3      3.0  False\n",
       "1  A1XI6NT41B6E6X  B00004ZCB3      5.0  False\n",
       "2  A2PMIMM3U3THM7  B00004ZCB3      5.0  False\n",
       "3  A1E9NFMLMVY61H  B00004ZCB4      5.0  False\n",
       "4   AXABTEYS7A4A8  B00004ZCB4      5.0  False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import cupyx.scipy.sparse\n",
    "import cudf\n",
    "import numpy as np\n",
    "\n",
    "ratings = cudf.read_csv(\"../data/ratings.csv\")\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we split into train and test, let's [factorize](https://docs.rapids.ai/api/cudf/stable/api.html#cudf.core.series.Series.factorize) our users and items. Our test set won't be effective if it doesn't use the same indexes as the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indexes, user_mapping = ratings[\"reviewerID\"].factorize()\n",
    "item_indexes, item_mapping = ratings[\"asin\"].factorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're building our sparse matrix, let's get a sense of our data along the way. First, let's see how many users there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192403"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_count = user_mapping.count()\n",
    "\n",
    "user_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost two hundred thousand! Okay, how about the items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_count = item_mapping.count()\n",
    "\n",
    "item_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much bigger than our toy problem with four users and five items. Using [todense](https://docs-cupy.chainer.org/en/stable/reference/generated/cupyx.scipy.sparse.coo_matrix.html#cupyx.scipy.sparse.coo_matrix.todense) will result in memory limitations, which in the real world is both frustrating and expensive.\n",
    "\n",
    "Instead, we'll need to rely on good testing. Let's break our ratings data down into train and test. Please fill in the `FIXME`s below, using the <a href=\"1-05_als_intro.ipynb\">previous lab</a> as a hint if needed. There is one `FIXME` for each input of the `get_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "train_indexes = ~ratings[\"valid\"]\n",
    "valid_indexes = ratings[\"valid\"]\n",
    "overall = ratings[\"overall\"]\n",
    "\n",
    "def get_dataset(data_selector, user_indexes, item_indexes, overall):\n",
    "    row = cp.asarray(user_indexes[data_selector])\n",
    "    column = cp.asarray(item_indexes[data_selector])\n",
    "    data = cp.asarray(overall[data_selector])\n",
    "    \n",
    "    sparse_data = cupyx.scipy.sparse.coo_matrix((data, (row, column)))\n",
    "    mask = cp.asarray([1 for _ in range(len(data))], dtype=np.float32)\n",
    "    sparse_mask = cupyx.scipy.sparse.coo_matrix((mask, (row, column)))\n",
    "    return row, column, data, sparse_data, sparse_mask\n",
    "\n",
    "train_row, train_column, train_data, train_sparse, train_mask = get_dataset(\n",
    "    train_indexes, user_indexes, item_indexes, overall)\n",
    "\n",
    "valid_row, valid_column, valid_data, valid_sparse, valid_mask = get_dataset(\n",
    "    valid_indexes, user_indexes, item_indexes, overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Alternating Least Squares\n",
    "\n",
    "It's time to initialize our embeddings. Let's copy over our functions from the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = 2\n",
    "shape = (user_count, item_count)\n",
    "\n",
    "def initalize_features(length, embeddings):\n",
    "    return cp.random.rand(embeddings, length) * 2 - 1\n",
    "\n",
    "user_features = initalize_features(shape[0], embeddings)\n",
    "item_features = initalize_features(shape[1], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(user_features, item_features, data, row, column):\n",
    "    predictions = (user_features[:, row] * item_features[:, column]).sum(axis=0)\n",
    "    mean_squared_error = ((predictions - data) ** 2).mean() ** 0.5\n",
    "    \n",
    "    return predictions, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ALS function below is incomplete. Please fill in the `FIXME`s to get it up and running. There is one `FIXME` for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als(values, mask, features, scale=0.01):\n",
    "    numerator = values.dot(features.T)\n",
    "    squared_features = (features ** 2).sum(axis=0)\n",
    "    denominator = scale + mask.dot(squared_features)\n",
    "\n",
    "    return (numerator / denominator[:, None]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How low can you go? Run the below cell multiple times to watch the RMSE fall. If the above `FIXME`s are correctly implemented, the RMSE can reach below `1.25`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.750533888189017\n",
      "RMSE: 4.817339361099724\n",
      "RMSE: 4.724361608649527\n",
      "RMSE: 3.8984390421321558\n",
      "RMSE: 2.1682555799806162\n",
      "RMSE: 1.5670989597845253\n",
      "RMSE: 1.3811294679964137\n",
      "RMSE: 1.3111891395134707\n",
      "RMSE: 1.2767704390718608\n",
      "RMSE: 1.2435773412612998\n",
      "RMSE: 1.223426514773521\n",
      "RMSE: 1.2200548272731337\n",
      "RMSE: 1.2195751822842507\n",
      "RMSE: 1.214767974358177\n",
      "RMSE: 1.2088431122969083\n",
      "RMSE: 1.205333822457222\n",
      "RMSE: 1.2041495747696696\n",
      "RMSE: 1.2039103129322577\n",
      "RMSE: 1.2038763506012367\n",
      "RMSE: 1.2038716780724825\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    user_features = als(train_sparse, train_mask, item_features)\n",
    "    item_features = als(train_sparse.T, train_mask.T, user_features)\n",
    "    predictions, error = rmse(\n",
    "        user_features, item_features, valid_data, valid_row, valid_column)\n",
    "\n",
    "    print (\"RMSE:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a human eyeball test to verify our prediction error. Do the predictions follow the same ranking order as the true ratings do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 5., 5., 5., 5., 5., 1., 5., 2., 5.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.37967407, 3.63454168, 5.19205639, 3.10017566, 4.73560372,\n",
       "       3.39309085, 4.17715775, 4.43286414, 3.22058951, 3.98031958])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wrap Up\n",
    "\n",
    "Congratulations on finishing this set of notebooks! We learned how to do two major types of Recommender Systems: Content-based and Collaborative. While we were able to achieve a lower RMSE with Collaborative Filtering, there are some situations were Content-based wins out. Have an idea what they are? Head back over to the notebook launcher to take a quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"../images/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
