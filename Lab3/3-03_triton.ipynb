{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./images/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton for Recommender Systems\n",
    "\n",
    "The [Triton Inference Server](https://github.com/triton-inference-server/server/blob/main/README.md#documentation) allows us to deploy our model to the web regardless of cloud provider, and it supports a number of different machine learning frameworks such as TensorFlow and PyTorch.\n",
    "\n",
    "## Objectives\n",
    "* Learn how to deploy a model to Triton\n",
    "  * [1. Deploy TensorFlow Model to Triton Inference Server](#1.-Deploy-TensorFlow-Model-to-Triton-Inference-Server)\n",
    "      * [1.1 Export a Model](#1.1-Export-a-Model)\n",
    "      * [1.2 Review exported files](#1.2-Review-exported-files)\n",
    "      * [1.3 Loading a Model](#1.3-Loading-a-Model)\n",
    "  * [2. Sent requests for predictions](#2.-Sent-requests-for-predictions)\n",
    "* Learn how to record deployment metrics\n",
    "  * [3. Server Metrics](#3.-Server-Metrics)\n",
    "\n",
    "## 1. Deploy TensorFlow Model to Triton Inference Server\n",
    "\n",
    "Our Triton server has already been launched to the web and is ready to make requests. First, we need to export the saved TensorFlow model from Lab 2 and generate the config file for Triton Inference Server. NVTabular provides an easy-to-use function, which manages both tasks.\n",
    "\n",
    "### 1.1 Export a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External dependencies\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import tritonhttpclient\n",
    "\n",
    "import cudf\n",
    "import tritonclient.grpc as grpcclient\n",
    "import nvtabular.inference.triton as nvt_triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unzip the model that we saved as a zip file in the previous notebook, and then load it to be able to use it in the NVTabular `export_tensorflow_model()` function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/task_2_model.zip\n",
      "   creating: task2_model/\n",
      "  inflating: task2_model/saved_model.pb  \n",
      "   creating: task2_model/variables/\n",
      "  inflating: task2_model/variables/variables.data-00000-of-00001  \n",
      "  inflating: task2_model/variables/variables.index  \n",
      "   creating: task2_model/assets/\n"
     ]
    }
   ],
   "source": [
    "!unzip data/task_2_model.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('task2_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will need the output name of the last layer to make predictions later, let's print them out using `model.output_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tf.__operators__.add']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can export the model to `model_repository`. This folder is shared between the docker container for the jupyter notebook and the docker container that runs Triton Inference Server. Therefore, Triton will have access to the model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_repository/wnd_tf/1/model.savedmodel/assets\n"
     ]
    }
   ],
   "source": [
    "import nvtabular\n",
    "\n",
    "# generate the TF saved model\n",
    "from nvtabular.inference.triton.ensemble import export_tensorflow_model\n",
    "\n",
    "tf_config = export_tensorflow_model(model, \"wnd_tf\", \"model_repository/wnd_tf\", version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To free GPU memory, we will restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Review exported files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the files `export_tensorflow_model` created. Triton expects [a specific directory structure](https://github.com/triton-inference-server/server/blob/main/docs/model_repository.md) for our models. The folder `/model_repository` is shared with our server, and it expects the following format:\n",
    "\n",
    "```<model_repository_path>/\n",
    "  <model-name>/\n",
    "    [config.pbtxt]\n",
    "    <version-name>/\n",
    "      [model.savedmodel]/\n",
    "        <tensorflow_saved_model_files>/\n",
    "          ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmodel_repository\u001b[00m\n",
      "└── \u001b[01;34mwnd_tf\u001b[00m\n",
      "    ├── \u001b[01;34m1\u001b[00m\n",
      "    │   └── \u001b[01;34mmodel.savedmodel\u001b[00m\n",
      "    │       ├── \u001b[01;34massets\u001b[00m\n",
      "    │       ├── saved_model.pb\n",
      "    │       └── \u001b[01;34mvariables\u001b[00m\n",
      "    │           ├── variables.data-00000-of-00001\n",
      "    │           └── variables.index\n",
      "    └── config.pbtxt\n",
      "\n",
      "5 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "!tree model_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the generated config file. It defines the input columns with datatype and dimensions and the output layer. Manually creating this config file can be complicated and NVTabular provides an easy function with `export_tensorflow_model` to deploy TensorFlow model to Triton.\n",
    "\n",
    "Triton needs a [config file](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) to understand how to interpret the model. Our `export_tensorflow_model` method is automaticall creating the config file and the required folder structure for us, so that we do not need to create it manually.\n",
    "\n",
    "The config file needs the following information:\n",
    "* name: The name of our model. Must be the same name as the parent folder.\n",
    "* platform: The type of framework serving the model.\n",
    "* input: The input our model expects.\n",
    "  * `name`: Should correspond with the model input name.\n",
    "  * `data_type`: Should correspond to the input's data type.\n",
    "  * `dims`: The dimensions of the *request* for the input, as in the dimensions of the data the user passes to us.\n",
    "  * `reshape`: How to reshape the data from the client to pass it to our model. In this case, the minimum dims from the client is `[1]`, but like Keras, Triton appends a dimension for batching. If our model expects `[batch_size]` as a dimension, we can reshape our data to `[]` (empty brackets) to account for that.\n",
    "* output: The output parameters of our model.\n",
    "  * `name`: Should correspond with the model output name. In this case, we're using the name automatically assigned by TensorFlow.\n",
    "  * `data_type`: Should correspond to the output's data type.\n",
    "  * `dims`: The dimensions of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"wnd_tf\"\n",
      "platform: \"tensorflow_savedmodel\"\n",
      "input {\n",
      "  name: \"brand_index\"\n",
      "  data_type: TYPE_INT64\n",
      "  dims: -1\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"category_0_2_index\"\n",
      "  data_type: TYPE_INT32\n",
      "  dims: -1\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"category_1_2_index\"\n",
      "  data_type: TYPE_INT32\n",
      "  dims: -1\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"item_index\"\n",
      "  data_type: TYPE_INT64\n",
      "  dims: -1\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"price_filled\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: -1\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"salesRank_Electronics\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: -1\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"user_index\"\n",
      "  data_type: TYPE_INT64\n",
      "  dims: -1\n",
      "  dims: 1\n",
      "}\n",
      "output {\n",
      "  name: \"tf.__operators__.add\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: -1\n",
      "  dims: 1\n",
      "}\n",
      "backend: \"tensorflow\"\n"
     ]
    }
   ],
   "source": [
    "!cat model_repository/wnd_tf/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Loading a Model\n",
    "\n",
    "Now, we can communicate with the Triton Inference Server and sent the request to load the model. We can verify this by using [curl](https://curl.haxx.se/) to make a `GET` request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "\u001b[1mContent-Length\u001b[0m: 0\n",
      "\u001b[1mContent-Type\u001b[0m: text/plain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!curl -i triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's build a client to connect to our server. This [InferenceServerClient](https://github.com/triton-inference-server/client) object is what we'll be using to talk to Triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonhttpclient\n",
    "\n",
    "import cudf\n",
    "import tritonclient.grpc as grpcclient\n",
    "import nvtabular.inference.triton as nvt_triton\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"triton:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that our server is ready to go by using [is_server_live](https://github.com/triton-inference-server/client/blob/12d8a2a7318ccb4a367a09a42b80feba53f3944a/src/python/library/tritonclient/grpc/__init__.py#L259). [get_model_repository_index](https://github.com/triton-inference-server/client/blob/12d8a2a7318ccb4a367a09a42b80feba53f3944a/src/python/library/tritonclient/grpc/__init__.py#L555) will also show what folders are in Triton's model repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.is_server_live()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '19'}>\n",
      "bytearray(b'[{\"name\":\"wnd_tf\"}]')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'wnd_tf'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is configured, let's get the model loaded! First, we'll create a version for our model. By default, Triton loads the version in the first listed folder, so we'll use `1` for our version number.\n",
    "\n",
    "Finally, we'll copy our model into the server.\n",
    "\n",
    "We've set Triton's [Model Control Mode](https://github.com/triton-inference-server/server/blob/main/docs/model_management.md#model-control-mode-explicit) to `EXPLICIT`, meaning, it's not going to automatically pick up the model placed in it's directory. This is done on line 13 of our `docker-compose.yml` file in the [previous lab](3-02_docker.ipynb). We could have used [POLL](https://github.com/triton-inference-server/server/blob/main/docs/model_management.md) in order to do this, but it's not immediate when checking for changes.\n",
    "\n",
    "In order to load our model, we'll use [load_model](https://github.com/triton-inference-server/client/blob/12d8a2a7318ccb4a367a09a42b80feba53f3944a/src/python/library/tritonclient/grpc/__init__.py#L601). When needed, we can use [unload_model](https://github.com/triton-inference-server/client/blob/12d8a2a7318ccb4a367a09a42b80feba53f3944a/src/python/library/tritonclient/grpc/__init__.py#L634) when we want to remove it from the Triton server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST v2/repository/models/wnd_tf/load, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'wnd_tf'\n"
     ]
    }
   ],
   "source": [
    "model_name = \"wnd_tf\"\n",
    "triton_client.load_model(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is loaded, we can use [get_model_metadata](https://github.com/triton-inference-server/client/blob/12d8a2a7318ccb4a367a09a42b80feba53f3944a/src/python/library/tritonclient/grpc/__init__.py#L429) to see our model's inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET v2/models/wnd_tf, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '577'}>\n",
      "bytearray(b'{\"name\":\"wnd_tf\",\"versions\":[\"1\"],\"platform\":\"tensorflow_savedmodel\",\"inputs\":[{\"name\":\"brand_index\",\"datatype\":\"INT64\",\"shape\":[-1,1]},{\"name\":\"category_0_2_index\",\"datatype\":\"INT32\",\"shape\":[-1,1]},{\"name\":\"category_1_2_index\",\"datatype\":\"INT32\",\"shape\":[-1,1]},{\"name\":\"item_index\",\"datatype\":\"INT64\",\"shape\":[-1,1]},{\"name\":\"price_filled\",\"datatype\":\"FP32\",\"shape\":[-1,1]},{\"name\":\"salesRank_Electronics\",\"datatype\":\"FP32\",\"shape\":[-1,1]},{\"name\":\"user_index\",\"datatype\":\"INT64\",\"shape\":[-1,1]}],\"outputs\":[{\"name\":\"tf.__operators__.add\",\"datatype\":\"FP32\",\"shape\":[-1,1]}]}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'wnd_tf',\n",
       " 'versions': ['1'],\n",
       " 'platform': 'tensorflow_savedmodel',\n",
       " 'inputs': [{'name': 'brand_index', 'datatype': 'INT64', 'shape': [-1, 1]},\n",
       "  {'name': 'category_0_2_index', 'datatype': 'INT32', 'shape': [-1, 1]},\n",
       "  {'name': 'category_1_2_index', 'datatype': 'INT32', 'shape': [-1, 1]},\n",
       "  {'name': 'item_index', 'datatype': 'INT64', 'shape': [-1, 1]},\n",
       "  {'name': 'price_filled', 'datatype': 'FP32', 'shape': [-1, 1]},\n",
       "  {'name': 'salesRank_Electronics', 'datatype': 'FP32', 'shape': [-1, 1]},\n",
       "  {'name': 'user_index', 'datatype': 'INT64', 'shape': [-1, 1]}],\n",
       " 'outputs': [{'name': 'tf.__operators__.add',\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [-1, 1]}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_client.get_model_metadata(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, time to shine! Let's make a request to our server!\n",
    "\n",
    "### 2. Sent requests for predictions\n",
    "\n",
    "We can use [InferInput](https://github.com/triton-inference-server/client/blob/12d8a2a7318ccb4a367a09a42b80feba53f3944a/src/python/library/tritonclient/grpc/__init__.py#L1449) to describe the tensors we'll be sending to the server. It needs the name of the input, the shape of the tensor we'll be passing to the server, and its datatype.\n",
    "\n",
    "Then, we can use [set_data_from_numpy](https://github.com/triton-inference-server/client/blob/12d8a2a7318ccb4a367a09a42b80feba53f3944a/src/python/library/tritonclient/grpc/__init__.py#L1513) to pass it a NumPy array.\n",
    "\n",
    "We'll use some fake data for now. The first row of our batch will have all `1`s and the second will have all `2`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST v2/models/wnd_tf/infer, headers {'Inference-Header-Content-Length': 759}\n",
      "b'{\"inputs\":[{\"name\":\"user_index\",\"shape\":[2,1],\"datatype\":\"INT64\",\"parameters\":{\"binary_data_size\":16}},{\"name\":\"item_index\",\"shape\":[2,1],\"datatype\":\"INT64\",\"parameters\":{\"binary_data_size\":16}},{\"name\":\"brand_index\",\"shape\":[2,1],\"datatype\":\"INT64\",\"parameters\":{\"binary_data_size\":16}},{\"name\":\"price_filled\",\"shape\":[2,1],\"datatype\":\"FP32\",\"parameters\":{\"binary_data_size\":8}},{\"name\":\"salesRank_Electronics\",\"shape\":[2,1],\"datatype\":\"FP32\",\"parameters\":{\"binary_data_size\":8}},{\"name\":\"category_0_2_index\",\"shape\":[2,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":8}},{\"name\":\"category_1_2_index\",\"shape\":[2,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":8}}],\"outputs\":[{\"name\":\"tf.__operators__.add\",\"parameters\":{\"binary_data\":false}}]}\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80?\\x00\\x00\\x00@\\x00\\x00\\x80?\\x00\\x00\\x00@\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00'\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '165'}>\n",
      "bytearray(b'{\"model_name\":\"wnd_tf\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"tf.__operators__.add\",\"datatype\":\"FP32\",\"shape\":[2,1],\"data\":[4.675514221191406,4.7014875411987309]}]}')\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "batch_size = 2\n",
    "inputs.append(tritonhttpclient.InferInput(\"user_index\", [batch_size, 1], \"INT64\"))\n",
    "inputs.append(tritonhttpclient.InferInput(\"item_index\", [batch_size, 1], \"INT64\"))\n",
    "inputs.append(tritonhttpclient.InferInput(\"brand_index\", [batch_size, 1], \"INT64\"))\n",
    "inputs.append(tritonhttpclient.InferInput(\"price_filled\", [batch_size, 1], \"FP32\"))\n",
    "inputs.append(tritonhttpclient.InferInput(\"salesRank_Electronics\", [batch_size, 1], \"FP32\"))\n",
    "inputs.append(tritonhttpclient.InferInput(\"category_0_2_index\", [batch_size, 1], \"INT32\"))\n",
    "inputs.append(tritonhttpclient.InferInput(\"category_1_2_index\", [batch_size, 1], \"INT32\"))\n",
    "\n",
    "inputs[0].set_data_from_numpy(np.array([[1], [2]], dtype=np.int64))\n",
    "inputs[1].set_data_from_numpy(np.array([[1], [2]], dtype=np.int64))\n",
    "inputs[2].set_data_from_numpy(np.array([[1], [2]], dtype=np.int64))\n",
    "inputs[3].set_data_from_numpy(np.array([[1.0], [2.0]], dtype=np.float32))\n",
    "inputs[4].set_data_from_numpy(np.array([[1.0], [2.0]], dtype=np.float32))\n",
    "inputs[5].set_data_from_numpy(np.array([[1], [2]], dtype=np.int32))\n",
    "inputs[6].set_data_from_numpy(np.array([[1], [2]], dtype=np.int32))\n",
    "\n",
    "outputs.append(\n",
    "    tritonhttpclient.InferRequestedOutput(\"tf.__operators__.add\", binary_data=False)\n",
    ")\n",
    "results = triton_client.infer(model_name, inputs, outputs=outputs).get_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll get a bunch of data returned from our response, but the important one is the `\"data\"` at the very end. That's our prediction from our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.675514221191406, 4.7014875411987305]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"outputs\"][0][\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a lot of work for only two predictions. Can we give it something meatier? We have loaded in the data from our previous labs. Let's try running our validation data from lab2 through the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (9,10,11,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>brand</th>\n",
       "      <th>category_0_0</th>\n",
       "      <th>category_0_1</th>\n",
       "      <th>category_0_2</th>\n",
       "      <th>category_0_3</th>\n",
       "      <th>category_1_0</th>\n",
       "      <th>...</th>\n",
       "      <th>user_index</th>\n",
       "      <th>item_index</th>\n",
       "      <th>brand_index</th>\n",
       "      <th>als_prediction</th>\n",
       "      <th>user_embed_0</th>\n",
       "      <th>user_embed_1</th>\n",
       "      <th>item_embed_0</th>\n",
       "      <th>item_embed_1</th>\n",
       "      <th>category_0_2_index</th>\n",
       "      <th>category_1_2_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>A236926FEQNZE5</td>\n",
       "      <td>B00006HVLW</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1390608000</td>\n",
       "      <td>BELKIN</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Computers &amp; Accessories</td>\n",
       "      <td>Cables &amp; Accessories</td>\n",
       "      <td>Surge Protectors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>55593</td>\n",
       "      <td>1685</td>\n",
       "      <td>307</td>\n",
       "      <td>3.031549</td>\n",
       "      <td>1.267751</td>\n",
       "      <td>-2.118827</td>\n",
       "      <td>0.668555</td>\n",
       "      <td>-1.117375</td>\n",
       "      <td>28</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>A9SRRW7T0HY70</td>\n",
       "      <td>B00006HVLW</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1395619200</td>\n",
       "      <td>BELKIN</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Computers &amp; Accessories</td>\n",
       "      <td>Cables &amp; Accessories</td>\n",
       "      <td>Surge Protectors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>155591</td>\n",
       "      <td>1685</td>\n",
       "      <td>307</td>\n",
       "      <td>4.308138</td>\n",
       "      <td>1.801603</td>\n",
       "      <td>-3.011068</td>\n",
       "      <td>0.699708</td>\n",
       "      <td>-1.169436</td>\n",
       "      <td>28</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>A17R3BS1KYP1OO</td>\n",
       "      <td>B00006HVLW</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1388707200</td>\n",
       "      <td>BELKIN</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Computers &amp; Accessories</td>\n",
       "      <td>Cables &amp; Accessories</td>\n",
       "      <td>Surge Protectors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11029</td>\n",
       "      <td>1685</td>\n",
       "      <td>307</td>\n",
       "      <td>3.478020</td>\n",
       "      <td>1.454458</td>\n",
       "      <td>-2.430878</td>\n",
       "      <td>0.574669</td>\n",
       "      <td>-0.960460</td>\n",
       "      <td>28</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>A3SOVP1ITVGD9A</td>\n",
       "      <td>B00007GQLU</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1384473600</td>\n",
       "      <td>Canon</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>Lenses</td>\n",
       "      <td>Camera Lenses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>142291</td>\n",
       "      <td>2065</td>\n",
       "      <td>573</td>\n",
       "      <td>4.575386</td>\n",
       "      <td>1.514265</td>\n",
       "      <td>-2.530830</td>\n",
       "      <td>0.679521</td>\n",
       "      <td>-1.135702</td>\n",
       "      <td>82</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A39ZPRN0LTNHFD</td>\n",
       "      <td>B000068P8W</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1363132800</td>\n",
       "      <td>Monster</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Computers &amp; Accessories</td>\n",
       "      <td>Cables &amp; Accessories</td>\n",
       "      <td>Cables &amp; Interconnects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>115847</td>\n",
       "      <td>1435</td>\n",
       "      <td>1970</td>\n",
       "      <td>4.266848</td>\n",
       "      <td>1.613634</td>\n",
       "      <td>-2.696908</td>\n",
       "      <td>0.729389</td>\n",
       "      <td>-1.219050</td>\n",
       "      <td>28</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID        asin  overall  unixReviewTime    brand category_0_0  \\\n",
       "32  A236926FEQNZE5  B00006HVLW      3.0      1390608000   BELKIN  Electronics   \n",
       "33   A9SRRW7T0HY70  B00006HVLW      4.0      1395619200   BELKIN  Electronics   \n",
       "34  A17R3BS1KYP1OO  B00006HVLW      3.0      1388707200   BELKIN  Electronics   \n",
       "64  A3SOVP1ITVGD9A  B00007GQLU      5.0      1384473600    Canon  Electronics   \n",
       "96  A39ZPRN0LTNHFD  B000068P8W      5.0      1363132800  Monster  Electronics   \n",
       "\n",
       "               category_0_1          category_0_2            category_0_3  \\\n",
       "32  Computers & Accessories  Cables & Accessories        Surge Protectors   \n",
       "33  Computers & Accessories  Cables & Accessories        Surge Protectors   \n",
       "34  Computers & Accessories  Cables & Accessories        Surge Protectors   \n",
       "64           Camera & Photo                Lenses           Camera Lenses   \n",
       "96  Computers & Accessories  Cables & Accessories  Cables & Interconnects   \n",
       "\n",
       "   category_1_0  ... user_index item_index brand_index  als_prediction  \\\n",
       "32          NaN  ...      55593       1685         307        3.031549   \n",
       "33          NaN  ...     155591       1685         307        4.308138   \n",
       "34          NaN  ...      11029       1685         307        3.478020   \n",
       "64          NaN  ...     142291       2065         573        4.575386   \n",
       "96          NaN  ...     115847       1435        1970        4.266848   \n",
       "\n",
       "    user_embed_0  user_embed_1  item_embed_0  item_embed_1  \\\n",
       "32      1.267751     -2.118827      0.668555     -1.117375   \n",
       "33      1.801603     -3.011068      0.699708     -1.169436   \n",
       "34      1.454458     -2.430878      0.574669     -0.960460   \n",
       "64      1.514265     -2.530830      0.679521     -1.135702   \n",
       "96      1.613634     -2.696908      0.729389     -1.219050   \n",
       "\n",
       "    category_0_2_index  category_1_2_index  \n",
       "32                  28                 101  \n",
       "33                  28                 101  \n",
       "34                  28                 101  \n",
       "64                  82                 101  \n",
       "96                  28                 101  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"./data/task_2_wide_and_deep.csv\")\n",
    "ratings = ratings[ratings[\"valid\"]]\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to be a little more efficient with our code this time. We'll use a `for` loop to construct our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST v2/models/wnd_tf/infer, headers {'Inference-Header-Content-Length': 777}\n",
      "b'{\"inputs\":[{\"name\":\"user_index\",\"shape\":[64,1],\"datatype\":\"INT64\",\"parameters\":{\"binary_data_size\":512}},{\"name\":\"item_index\",\"shape\":[64,1],\"datatype\":\"INT64\",\"parameters\":{\"binary_data_size\":512}},{\"name\":\"brand_index\",\"shape\":[64,1],\"datatype\":\"INT64\",\"parameters\":{\"binary_data_size\":512}},{\"name\":\"price_filled\",\"shape\":[64,1],\"datatype\":\"FP32\",\"parameters\":{\"binary_data_size\":256}},{\"name\":\"salesRank_Electronics\",\"shape\":[64,1],\"datatype\":\"FP32\",\"parameters\":{\"binary_data_size\":256}},{\"name\":\"category_0_2_index\",\"shape\":[64,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":256}},{\"name\":\"category_1_2_index\",\"shape\":[64,1],\"datatype\":\"INT32\",\"parameters\":{\"binary_data_size\":256}}],\"outputs\":[{\"name\":\"tf.__operators__.add\",\"parameters\":{\"binary_data\":false}}]})\\xd9\\x00\\x00\\x00\\x00\\x00\\x00\\xc7_\\x02\\x00\\x00\\x00\\x00\\x00\\x15+\\x00\\x00\\x00\\x00\\x00\\x00\\xd3+\\x02\\x00\\x00\\x00\\x00\\x00\\x87\\xc4\\x01\\x00\\x00\\x00\\x00\\x00d\\xd8\\x00\\x00\\x00\\x00\\x00\\x00\\x82^\\x02\\x00\\x00\\x00\\x00\\x00\\xb4\\xe0\\x00\\x00\\x00\\x00\\x00\\x00\\xd0X\\x00\\x00\\x00\\x00\\x00\\x00\\xf8}\\x01\\x00\\x00\\x00\\x00\\x00\\x93\\xf4\\x00\\x00\\x00\\x00\\x00\\x00\\xfa\\xa2\\x02\\x00\\x00\\x00\\x00\\x00\\x19\\xc5\\x02\\x00\\x00\\x00\\x00\\x00\\x88\\x80\\x00\\x00\\x00\\x00\\x00\\x00|\\xee\\x00\\x00\\x00\\x00\\x00\\x00+\\x19\\x00\\x00\\x00\\x00\\x00\\x000\\xc5\\x01\\x00\\x00\\x00\\x00\\x00\\xb0Y\\x02\\x00\\x00\\x00\\x00\\x00\\xddq\\x01\\x00\\x00\\x00\\x00\\x00\\xf9\\x08\\x00\\x00\\x00\\x00\\x00\\x00_&\\x00\\x00\\x00\\x00\\x00\\x00\\x1cq\\x01\\x00\\x00\\x00\\x00\\x00,\\xd6\\x00\\x00\\x00\\x00\\x00\\x00a\\x94\\x01\\x00\\x00\\x00\\x00\\x00\\x9e\\x12\\x00\\x00\\x00\\x00\\x00\\x00\\x86h\\x00\\x00\\x00\\x00\\x00\\x00\\x99\\x16\\x00\\x00\\x00\\x00\\x00\\x00(\\x9e\\x02\\x00\\x00\\x00\\x00\\x00\\x91\\xb1\\x00\\x00\\x00\\x00\\x00\\x00\\xe7\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\x89\\xb9\\x02\\x00\\x00\\x00\\x00\\x00k\\x1e\\x01\\x00\\x00\\x00\\x00\\x00\\xf3%\\x01\\x00\\x00\\x00\\x00\\x00[U\\x00\\x00\\x00\\x00\\x00\\x00\\x0f\\xd3\\x02\\x00\\x00\\x00\\x00\\x00\\xb71\\x01\\x00\\x00\\x00\\x00\\x00oc\\x00\\x00\\x00\\x00\\x00\\x00\\xcd\\xee\\x02\\x00\\x00\\x00\\x00\\x00\\xee\\xcf\\x00\\x00\\x00\\x00\\x00\\x00\\x17\\xb8\\x00\\x00\\x00\\x00\\x00\\x00\\x9b3\\x02\\x00\\x00\\x00\\x00\\x00\\xcb\\x8e\\x01\\x00\\x00\\x00\\x00\\x00\\xe0|\\x00\\x00\\x00\\x00\\x00\\x00\\xfc?\\x00\\x00\\x00\\x00\\x00\\x00\\xbe\\xca\\x01\\x00\\x00\\x00\\x00\\x00v\\x0c\\x02\\x00\\x00\\x00\\x00\\x00\\x12\\xbc\\x01\\x00\\x00\\x00\\x00\\x00U\\\\\\x02\\x00\\x00\\x00\\x00\\x00\\x9b \\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\xdd\\x01\\x00\\x00\\x00\\x00\\x00\\xea\\x8f\\x02\\x00\\x00\\x00\\x00\\x00\\xf8`\\x01\\x00\\x00\\x00\\x00\\x00\\x18T\\x00\\x00\\x00\\x00\\x00\\x00\\x96\\xaa\\x02\\x00\\x00\\x00\\x00\\x00Zj\\x02\\x00\\x00\\x00\\x00\\x00\\x8b\\xb4\\x02\\x00\\x00\\x00\\x00\\x00H\\x85\\x02\\x00\\x00\\x00\\x00\\x00\"\\xd9\\x01\\x00\\x00\\x00\\x00\\x00D\\x10\\x02\\x00\\x00\\x00\\x00\\x00YO\\x02\\x00\\x00\\x00\\x00\\x00Z\\xd5\\x02\\x00\\x00\\x00\\x00\\x00]\\xc3\\x02\\x00\\x00\\x00\\x00\\x00\\x99\\xfa\\x00\\x00\\x00\\x00\\x00\\x00.\\x85\\x00\\x00\\x00\\x00\\x00\\x00\\x95\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x95\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x95\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x11\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x9b\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x9b\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x9b\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x94\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x94\\n\\x00\\x00\\x00\\x00\\x00\\x00O\\t\\x00\\x00\\x00\\x00\\x00\\x00O\\t\\x00\\x00\\x00\\x00\\x00\\x00.\\x02\\x00\\x00\\x00\\x00\\x00\\x00.\\x02\\x00\\x00\\x00\\x00\\x00\\x00.\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\n\\x00\\x00\\x00\\x00\\x00\\x00{\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\xab\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xab\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xab\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xab\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xab\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xab\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xf4\\x05\\x00\\x00\\x00\\x00\\x00\\x006\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x9c\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xc6\\x00\\x00\\x00\\x00\\x00\\x00\\x00O\\x01\\x00\\x00\\x00\\x00\\x00\\x00O\\x01\\x00\\x00\\x00\\x00\\x00\\x00O\\x01\\x00\\x00\\x00\\x00\\x00\\x00O\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xd7\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xd7\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xd7\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x9e\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x9e\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x90\\x03\\x00\\x00\\x00\\x00\\x00\\x00j\\x04\\x00\\x00\\x00\\x00\\x00\\x00j\\x04\\x00\\x00\\x00\\x00\\x00\\x00j\\x04\\x00\\x00\\x00\\x00\\x00\\x00j\\x04\\x00\\x00\\x00\\x00\\x00\\x00j\\x04\\x00\\x00\\x00\\x00\\x00\\x00j\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\xf0\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\xf0\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\xf0\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\n\\x00\\x00\\x00\\x00\\x00\\x004\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x88\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x89\\x04\\x00\\x00\\x00\\x00\\x00\\x00=\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\xfc\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\xfc\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\xfc\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x9a\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x9a\\x05\\x00\\x00\\x00\\x00\\x00\\x00.\\x06\\x00\\x00\\x00\\x00\\x00\\x00.\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\xfc\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x0f\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x10\\x05\\x00\\x00\\x00\\x00\\x00\\x003\\x01\\x00\\x00\\x00\\x00\\x00\\x003\\x01\\x00\\x00\\x00\\x00\\x00\\x003\\x01\\x00\\x00\\x00\\x00\\x00\\x00=\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xb2\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\xb2\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\xb2\\x07\\x00\\x00\\x00\\x00\\x00\\x00=\\x00\\x00\\x00\\x00\\x00\\x00\\x00=\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x83\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x83\\x00\\x00\\x00\\x00\\x00\\x00\\x00i\\x01\\x00\\x00\\x00\\x00\\x00\\x00i\\x01\\x00\\x00\\x00\\x00\\x00\\x00i\\x01\\x00\\x00\\x00\\x00\\x00\\x00=\\x02\\x00\\x00\\x00\\x00\\x00\\x00=\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\xd1\\n\\x00\\x00\\x00\\x00\\x00\\x00=\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xcc\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00=\\x02\\x00\\x00\\x00\\x00\\x00\\x00=\\x02\\x00\\x00\\x00\\x00\\x00\\x00=\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00f\\x06\\x00\\x00\\x00\\x00\\x00\\x00f\\x06\\x00\\x00\\x00\\x00\\x00\\x00f\\x06\\x00\\x00\\x00\\x00\\x00\\x00f\\x06\\x00\\x00\\x00\\x00\\x00\\x00f\\x06\\x00\\x00\\x00\\x00\\x00\\x00f\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\xc1\\x0b\\x00\\x00\\x00\\x00\\x00\\x00=\\x02\\x00\\x00\\x00\\x00\\x00\\x00=\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xb2\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xe0\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\xe0\\x02\\x00\\x00\\x00\\x00\\x00\\x003\\x01\\x00\\x00\\x00\\x00\\x00\\x003\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x003\\x01\\x00\\x00\\x00\\x00\\x00\\x00i\\x01\\x00\\x00\\x00\\x00\\x00\\x00B\\x147\\xbfB\\x147\\xbfB\\x147\\xbf\\xcc\\xea\\xf1?\\x07;8\\xbf\\x07;8\\xbf\\x07;8\\xbf\\x1a*\\xeb>\\x1a*\\xeb>\\xe5\\x1f(\\xbf\\xe5\\x1f(\\xbf\\x982S\\xbe\\x982S\\xbe\\x982S\\xbe\\xfd\\x9fJ>\\xfd\\x9fJ>\\xd5\\xfa\\xf4>*\\xa7\\x9a>*\\xa7\\x9a>*\\xa7\\x9a>*\\xa7\\x9a>*\\xa7\\x9a>*\\xa7\\x9a>\\xd3j\\x9b\\xbfF\\xf7X?\\xf2<R\\xbf\\xf9\\xeaY?cO\\x82\\xbfcO\\x82\\xbfcO\\x82\\xbfcO\\x82\\xbf\\x0e\\xad\\x83>\\x0e\\xad\\x83>,m\\t@,m\\t@,m\\t@\\xecE\\x13?\\xecE\\x13?{k\\xba=\\x97\\x11\\xa6?\\x97\\x11\\xa6?\\x97\\x11\\xa6?\\x97\\x11\\xa6?\\x97\\x11\\xa6?\\x97\\x11\\xa6?\\xbaD1\\xbf\\xbaD1\\xbf\\xbaD1\\xbf\\xfd\\x9fJ>\\xfd\\x9fJ>\\x04\\xd1{?\\x81\\x9e2?\\xdd\\xd2\\x80\\xbf0\\xa1\\xff\\xbe\\xaa-K>\\xaa-K>\\xaa-K>\\x95\\x07\\xad>\\x95\\x07\\xad>-\\x8e9\\xbf-\\x8e9\\xbf\\xaa-K>\\xf9\\xcb|\\xbf\\x01\\x1bF\\xbf\\xf6\\x02\\xa9>\\xcf\\x02\\xa9>\\xa8\\x02\\xa9>\\x13\\xf8\\xa8>&\\xf9\\xa8>\\xfe\\xf8\\xa8>\\xd7\\xf8\\xa8>\\xa3\\x08\\xa9>|\\x08\\xa9>\\x88\\xe1\\xa8>\\xaf\\xe1\\xa8>\\x19\\xf0\\xa8>\\xf2\\xef\\xa8>\\xcb\\xef\\xa8>\\xdc\\xf2\\xa8>\\xb5\\xf2\\xa8>\\x11\\x13\\xa9>)B\\xa9>\\xdaA\\xa9>\\xb3A\\xa9>\\x8cA\\xa9>dA\\xa9>=A\\xa9>\\x9dD\\xa9>U!\\xa9>\\xe8\\x14\\xa9>\\x84\\x17\\xa9>f$\\xa9>?$\\xa9>\\x18$\\xa9>\\xf0#\\xa9>\\x02\\'\\xa9>\\xda&\\xa9>\\x16\\x85\\xa8>\\xed\\x86\\xa8>\\x15\\x87\\xa8>B\\x81\\xa8>\\x1a\\x81\\xa8>\\t\\x93\\xa8>\"\\x8e\\xa8>\\xfa\\x8d\\xa8>\\xd3\\x8d\\xa8>\\xac\\x8d\\xa8>\\x85\\x8d\\xa8>]\\x8d\\xa8>\\x9cu\\xa8>tu\\xa8>Mu\\xa8>\\xa3\\xc1\\xa8>U\\xc1\\xa8>\\xbd\\xd1\\xa8>\\x12\\xca\\xa8>\\xeb\\xc9\\xa8>\\'\\x9d\\xa8>\\x8a\\xb1\\xa8>c\\xb1\\xa8><\\xb1\\xa8>x\\xb0\\xa8>Q\\xb0\\xa8>\\x1a\\xff\\xa9>\\xf3\\xfe\\xa9>\\x1d\\xd2\\xa9>o\\xcc\\xa9>\\xa9\\xcf\\xa9>\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00R\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00P\\x00\\x00\\x00P\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00R\\x00\\x00\\x00$\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00R\\x00\\x00\\x00R\\x00\\x00\\x00R\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00P\\x00\\x00\\x00P\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00e\\x00\\x00\\x00'\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '1289'}>\n",
      "bytearray(b'{\"model_name\":\"wnd_tf\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"tf.__operators__.add\",\"datatype\":\"FP32\",\"shape\":[64,1],\"data\":[3.1596760749816896,4.880390644073486,3.62222957611084,4.547699451446533,4.229131698608398,4.535053253173828,4.195171356201172,4.84528923034668,4.874727725982666,4.3021697998046879,3.89747953414917,4.419257164001465,3.4447219371795656,4.7295756340026859,4.705485820770264,4.066901683807373,4.508839130401611,4.877975940704346,4.268950462341309,5.029654502868652,4.413246154785156,4.371448993682861,4.860851764678955,3.0594370365142824,4.756439208984375,3.869346857070923,4.933938980102539,4.367003440856934,4.573550224304199,3.7848050594329836,4.831972122192383,4.697257041931152,3.5304486751556398,4.901251792907715,4.818763732910156,4.868161201477051,4.86084508895874,4.275758743286133,4.2849860191345219,3.662626266479492,4.24597692489624,4.716452121734619,4.7339019775390629,2.879286766052246,2.490988254547119,4.742992877960205,4.893109321594238,4.897514820098877,4.696949481964111,5.120490550994873,4.347268581390381,4.721637725830078,4.751364707946777,4.4783735275268559,4.713237762451172,2.809736490249634,4.305532932281494,4.297007083892822,4.391058444976807,4.244709491729736,3.2851386070251467,4.0615363121032719,4.8753743171691898,4.675339698791504]}]}')\n",
      "\n",
      "prediction results:\n",
      " [3.1596760749816895, 4.880390644073486, 3.62222957611084, 4.547699451446533, 4.229131698608398, 4.535053253173828, 4.195171356201172, 4.84528923034668, 4.874727725982666, 4.3021697998046875, 3.89747953414917, 4.419257164001465, 3.4447219371795654, 4.7295756340026855, 4.705485820770264, 4.066901683807373, 4.508839130401611, 4.877975940704346, 4.268950462341309, 5.029654502868652, 4.413246154785156, 4.371448993682861, 4.860851764678955, 3.0594370365142822, 4.756439208984375, 3.869346857070923, 4.933938980102539, 4.367003440856934, 4.573550224304199, 3.7848050594329834, 4.831972122192383, 4.697257041931152, 3.5304486751556396, 4.901251792907715, 4.818763732910156, 4.868161201477051, 4.86084508895874, 4.275758743286133, 4.2849860191345215, 3.662626266479492, 4.24597692489624, 4.716452121734619, 4.7339019775390625, 2.879286766052246, 2.490988254547119, 4.742992877960205, 4.893109321594238, 4.897514820098877, 4.696949481964111, 5.120490550994873, 4.347268581390381, 4.721637725830078, 4.751364707946777, 4.4783735275268555, 4.713237762451172, 2.809736490249634, 4.305532932281494, 4.297007083892822, 4.391058444976807, 4.244709491729736, 3.2851386070251465, 4.0615363121032715, 4.8753743171691895, 4.675339698791504]\n"
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "    ('user_index', \"INT64\"),\n",
    "    ('item_index', \"INT64\"),\n",
    "    ('brand_index', \"INT64\"),\n",
    "    ('price_filled', \"FP32\"),\n",
    "    ('salesRank_Electronics', \"FP32\"),\n",
    "    ('category_0_2_index', \"INT32\"),\n",
    "    ('category_1_2_index', \"INT32\")\n",
    "]\n",
    "\n",
    "dtypes = {\n",
    "    \"INT32\": np.int32,\n",
    "    \"INT64\": np.int64,\n",
    "    \"FP32\": np.float32\n",
    "}\n",
    "\n",
    "inputs = []\n",
    "batch_size = 64\n",
    "for column in columns:\n",
    "    name = column[0]\n",
    "    dtype = dtypes[column[1]]\n",
    "    data = np.expand_dims(np.array(ratings.head(batch_size)[name], dtype=dtype), axis=-1)\n",
    "    inputs.append(tritonhttpclient.InferInput(name, [batch_size, 1], column[1]))\n",
    "    inputs[-1].set_data_from_numpy(data)\n",
    "\n",
    "results = triton_client.infer(model_name, inputs, outputs=outputs).get_response()\n",
    "\n",
    "print(\"\\nprediction results:\\n\", results[\"outputs\"][0][\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Server Metrics\n",
    "\n",
    "Not only can we scale serving our data, but we can also gather metrics on our model as well. This is crucial, finding the right metric to optimize for with recommender systems is not a trivial task. Check out this [great paper](https://www.kdd.org/exploration_files/19-1-Article3.pdf) explaining common pitfalls.\n",
    "\n",
    "The short version is this:\n",
    "* Recommender systems create a feedback loop between users and recommendations. Popular items train our models that these are good recommendations, thus serving them to more users and perpetuating the loop.\n",
    "* Try to avoid metrics that are biased by human behavior. For instance, click through rate is one commonly used in the advertisement space, but if not careful, using this will train the model which position on a web page is popular as opposed to the content.\n",
    "\n",
    "At the end of the day, the goal is to increase user engagement. Triton automatically serves usage metrics using [Prometheus](https://prometheus.io/). Copy and paste the URL (web address) for this notebook and set it to `my_url` below. Run the cell to see the metrics for our model. [Here](https://github.com/triton-inference-server/server/blob/main/docs/metrics.md) is a list of available metrics, but a good one to start with is `nv_inference_count` which displays how many predictions have been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"500\"\n",
       "            src=\"http://dli-337469e8f67f-e74638.aws.labs.courses.nvidia.com:9090/graph\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb7f37e8f10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "my_url = \"http://dli-337469e8f67f-e74638.aws.labs.courses.nvidia.com/lab/lab/tree/3-03_triton.ipynb\"\n",
    "prometheus_url = my_url.rsplit(\".com\", 1)[0] + \".com:9090/graph\"\n",
    "IPython.display.IFrame(prometheus_url, width=700, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up\n",
    "\n",
    "We can take this a little further and hook these results into a service like [Grafana](https://grafana.com/) as explained in [this excellent blog post](https://blog.einstein.ai/benchmarking-tensorrt-inference-server/) by the SalesForce team, but for now, we have all the pieces to build an end-to-end recommender system.\n",
    "\n",
    "Feeling ready? Head on over to [the next lab](3-04_assessment.ipynb) to put these new skills into action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./images/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
